{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d72f4f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd0f12f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = ''\n",
    "session = vk.Session(access_token=token)  # VK API authorization\n",
    "vk_api = vk.API(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d6cb53d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments_from_vk_group(owner_id):\n",
    "    \n",
    "    #getting the ids of 100 posts\n",
    "    group_posts_info = vk_api.wall.get(owner_id = owner_id, v = 5.95, count = 100)\n",
    "    group_post_ids = []\n",
    "    group_posts_items = group_posts_info['items']\n",
    "    for i in group_posts_items:\n",
    "      group_post_ids.append(i['id'])\n",
    "\n",
    "    #getting the comment from these posts\n",
    "    group_comments = []\n",
    "    for group_post_id in group_post_ids:\n",
    "      group_comments_info = vk_api.wall.getComments(owner_id = owner_id, post_id = group_post_id, v = 5.95, count = 100)\n",
    "      group_comments_items = group_comments_info['items']\n",
    "      for i in group_comments_items:\n",
    "        group_comments.append(i['text'])\n",
    "    print(f'Загружено {len(group_comments)} комментариев')\n",
    "    group_comments = pd.DataFrame(group_comments)\n",
    "    return group_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4e6dcaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружено 3978 комментариев\n"
     ]
    }
   ],
   "source": [
    "#Downloading the comments from Lentach group\n",
    "\n",
    "lentach_comments = get_comments_from_vk_group(-29534144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c8ac461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Loading labeled dataset and splitting 80/20 train/test\n",
    "\n",
    "training = pd.read_csv('labeled_2ch_pikabu.csv')\n",
    "train_data = training.comment\n",
    "train_labels = training.toxic\n",
    "\n",
    "first_training_data, first_test_data, first_training_labels, first_test_labels = train_test_split(train_data, train_labels, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f8aacd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Loading tensorflow libs and bert model\n",
    "\n",
    "tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1'\n",
    "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b8b4a3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1\n",
      "Epoch 1/7\n",
      "361/361 [==============================] - 1128s 3s/step - loss: 0.7432 - binary_accuracy: 0.6323 - f1_m: 0.2302 - precision_m: 0.4458 - recall_m: 0.2516\n",
      "Epoch 2/7\n",
      "361/361 [==============================] - 1132s 3s/step - loss: 0.5274 - binary_accuracy: 0.7401 - f1_m: 0.4642 - precision_m: 0.7390 - recall_m: 0.3726\n",
      "Epoch 3/7\n",
      "361/361 [==============================] - 1129s 3s/step - loss: 0.4641 - binary_accuracy: 0.7830 - f1_m: 0.5899 - precision_m: 0.7848 - recall_m: 0.4990\n",
      "Epoch 4/7\n",
      "361/361 [==============================] - 1545s 4s/step - loss: 0.4204 - binary_accuracy: 0.8108 - f1_m: 0.6553 - precision_m: 0.8128 - recall_m: 0.5705\n",
      "Epoch 5/7\n",
      "361/361 [==============================] - 1148s 3s/step - loss: 0.3996 - binary_accuracy: 0.8221 - f1_m: 0.6719 - precision_m: 0.8321 - recall_m: 0.5875\n",
      "Epoch 6/7\n",
      "361/361 [==============================] - 1131s 3s/step - loss: 0.3790 - binary_accuracy: 0.8325 - f1_m: 0.7019 - precision_m: 0.8331 - recall_m: 0.6275\n",
      "Epoch 7/7\n",
      "361/361 [==============================] - 1136s 3s/step - loss: 0.3702 - binary_accuracy: 0.8386 - f1_m: 0.7168 - precision_m: 0.8466 - recall_m: 0.6415\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 2885\n  y sizes: 2884\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-b697fe818b8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfirst_training_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfirst_training_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfirst_test_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfirst_test_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Loss: {loss}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1464\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1465\u001b[0m         \u001b[1;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1466\u001b[1;33m         data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[0;32m   1467\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1468\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1381\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_cluster_coordinator\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1382\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1383\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1138\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1139\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1140\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m     \u001b[0m_check_data_cardinality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[1;31m# If batch_size is not passed but steps is, calculate from the input data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1647\u001b[0m           label, \", \".join(str(i.shape[0]) for i in tf.nest.flatten(single_data)))\n\u001b[0;32m   1648\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1649\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 2885\n  y sizes: 2884\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "#Setting a model\n",
    "\n",
    "def build_classifier_model():\n",
    "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "  encoder_inputs = preprocessing_layer(text_input)\n",
    "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "  outputs = encoder(encoder_inputs)\n",
    "  net = outputs['pooled_output']\n",
    "  net = tf.keras.layers.Dropout(0.1)(net)\n",
    "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "  return tf.keras.Model(text_input, net)\n",
    "\n",
    "classifier_model = build_classifier_model()\n",
    "\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "binary_accuracy=tf.metrics.BinaryAccuracy()\n",
    "\n",
    "epochs = 7\n",
    "steps_per_epoch = 15000/32\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')\n",
    "\n",
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=[binary_accuracy,f1_m,precision_m, recall_m])\n",
    "\n",
    "print(f'Training model with {tfhub_handle_encoder}')\n",
    "history = classifier_model.fit(x=first_training_data, y=first_training_labels, epochs=epochs)\n",
    "\n",
    "loss, accuracy = classifier_model.evaluate(x=first_test_data, y=first_test_labels)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "history_dict = history.history\n",
    "print(history_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c58f365e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/91 [==============================] - 82s 879ms/step - loss: 0.3443 - binary_accuracy: 0.8561 - f1_m: 0.7551 - precision_m: 0.8724 - recall_m: 0.6830\n",
      "[0.34427452087402344, 0.8560526967048645, 0.7551085948944092, 0.8723870515823364, 0.6829580068588257]\n"
     ]
    }
   ],
   "source": [
    "#Test split results\n",
    "\n",
    "print(classifier_model.evaluate(x=first_test_data, y=first_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3f94a4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uploading test data with predictions\n",
    "\n",
    "test_data_df = pd.DataFrame(first_test_data)\n",
    "test_data_df.reset_index(drop=True, inplace=True)\n",
    "test_data_df['labels'] = pd.DataFrame(first_test_labels)\n",
    "test_data_df['predictions'] = pd.DataFrame(predictions)\n",
    "first_test_labels.reset_index(drop=True, inplace=True)\n",
    "test_data_df\n",
    "\n",
    "test_data_df.to_excel('test_df_second.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9909dd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>мой палец: *чуть влажный*\\n\\nсканер отпечатка ...</td>\n",
       "      <td>-1.165070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ахренкть, адмен украл мой коммент к клмменту, ...</td>\n",
       "      <td>-1.956791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>У топы был классный видос на эту тему</td>\n",
       "      <td>1.200571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>чтобы не попасть в тюрьму носите перчатки и маски</td>\n",
       "      <td>-0.626574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Первая два три четыре пять</td>\n",
       "      <td>0.505516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4117</th>\n",
       "      <td>интересно, а какие страны входят теперь в сбп?...</td>\n",
       "      <td>-0.095713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4118</th>\n",
       "      <td>Ну и на ху я азиатам лёд?</td>\n",
       "      <td>1.460273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4119</th>\n",
       "      <td>Лишившись льдов в горах лишатся рек . Узбекист...</td>\n",
       "      <td>1.627323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4120</th>\n",
       "      <td>Нужно реорганизовывать контроль цифровых систе...</td>\n",
       "      <td>-0.012236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4121</th>\n",
       "      <td>В офшор с мобилы, а удобно. \\nТеперь не придет...</td>\n",
       "      <td>-1.839131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4122 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0  prediction\n",
       "0     мой палец: *чуть влажный*\\n\\nсканер отпечатка ...   -1.165070\n",
       "1     Ахренкть, адмен украл мой коммент к клмменту, ...   -1.956791\n",
       "2                 У топы был классный видос на эту тему    1.200571\n",
       "3     чтобы не попасть в тюрьму носите перчатки и маски   -0.626574\n",
       "4                            Первая два три четыре пять    0.505516\n",
       "...                                                 ...         ...\n",
       "4117  интересно, а какие страны входят теперь в сбп?...   -0.095713\n",
       "4118                          Ну и на ху я азиатам лёд?    1.460273\n",
       "4119  Лишившись льдов в горах лишатся рек . Узбекист...    1.627323\n",
       "4120  Нужно реорганизовывать контроль цифровых систе...   -0.012236\n",
       "4121  В офшор с мобилы, а удобно. \\nТеперь не придет...   -1.839131\n",
       "\n",
       "[4122 rows x 2 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Real world data classification (unlabeled comments from Lentach group)\n",
    "\n",
    "lentach_comments = pd.DataFrame(lentach_comments)\n",
    "lentach_comments\n",
    "\n",
    "lentach_comments = lentach_comments.drop(['prediction'], axis = 1)\n",
    "lentach_comments\n",
    "\n",
    "lentach_predictions = classifier_model.predict(lentach_comments)\n",
    "lentach_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "af6b7e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export real world data predictions\n",
    "\n",
    "lentach_comments['prediction'] = lentach_predictions\n",
    "\n",
    "lentach_comments.to_excel('second_try_lentach_predictions.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f5c88327",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.save('bert_for_toxic_classification.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
